{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6656a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 测试torch安装是否成功\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0cc680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "12\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([100,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11])\n",
      "tensor([[100,   1,   2,   3],\n",
      "        [  4,   5,   6,   7],\n",
      "        [  8,   9,  10,  11]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor, 张量，零维是标量，一维是矢量，二维是矩阵等\n",
    "x = torch.arange(12)\n",
    "print(x)\n",
    "print(x.shape)  # 张量形状\n",
    "print(x.numel())  # 张量元素总数\n",
    "y = x.reshape(3, 4)  # 就地操作\n",
    "print(y)\n",
    "# 查看x中的元素变化，y中的会不会变\n",
    "x[0] = 100\n",
    "print(x)\n",
    "print(y)  # y中的元素也变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7d82b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[ 0.6815,  0.0349, -1.1498,  1.1388],\n",
      "        [ 0.9753,  1.2014, -0.9877,  0.5758],\n",
      "        [-1.7291,  2.2944, -0.3217,  2.0213]])\n",
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 全零张量\n",
    "print(torch.zeros((2, 3, 4)))\n",
    "# 全一张量\n",
    "print(torch.ones(3, 2, 4))\n",
    "# 高斯分布\n",
    "print(torch.randn(3, 4))\n",
    "# 使用列表初始化\n",
    "print(torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e537f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4.])\n",
      "tensor([4., 6., 8.])\n",
      "tensor([2., 3., 4.])\n",
      "tensor([ 2.7183,  7.3891, 20.0855])\n"
     ]
    }
   ],
   "source": [
    "# 运算符\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(x + 1)\n",
    "y = torch.tensor([3.0, 4.0, 5.0])\n",
    "print(x + y)\n",
    "z = torch.tensor([1.0])\n",
    "print(x + z)\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算\n",
    "# 求幂等运算\n",
    "print(torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd8e3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
      "tensor([[False,  True, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "tensor(66.)\n"
     ]
    }
   ],
   "source": [
    "# 张量拼接\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(torch.cat((x, y), dim=0))\n",
    "print(torch.cat((x, y), dim=1))\n",
    "print(x == y)\n",
    "print(x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b320b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([[0, 1]])\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# 广播机制\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a)\n",
    "print(b)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d793bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "# 索引和切片，依旧是前开后闭区间\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "print(x[-1])\n",
    "print(x[1:3])\n",
    "print(x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae21f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2460900698768\n",
      "id(y): 2460901226352\n",
      "id(y): 2460901226352\n"
     ]
    }
   ],
   "source": [
    "# 原地更新\n",
    "print(id(x))  # 查看张量x的id\n",
    "y = torch.zeros_like(x)\n",
    "print('id(y):', id(y))\n",
    "y[:] = x + x\n",
    "print('id(y):', id(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c8b1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1000., 1000., 1000., 1000.],\n",
      "        [   4.,    5.,    6.,    7.],\n",
      "        [   8.,    9.,   10.,   11.]]) 2460900698768\n",
      "[[1000. 1000. 1000. 1000.]\n",
      " [   4.    5.    6.    7.]\n",
      " [   8.    9.   10.   11.]] 2460915921520\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]]) 2460903266080\n",
      "tensor([[1000., 1000., 1000., 1000.],\n",
      "        [   4.,    5.,    6.,    7.],\n",
      "        [   8.,    9.,   10.,   11.]]) 2460879237824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch和numpy互相转换，共享内存\n",
    "y = x.numpy()  # 共享内存\n",
    "z = torch.tensor(y)  # 不共享内存\n",
    "u = torch.from_numpy(y)  # 共享内存\n",
    "print(type(y))\n",
    "print(type(z))\n",
    "y[0] = 1000\n",
    "print(x, id(x))  # 即使id不同，依旧共享内存\n",
    "print(y, id(y))\n",
    "print(z, id(z))\n",
    "print(u, id(u))\n",
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7a5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join(\"./\", \"data\"), exist_ok=True)\n",
    "data_file = os.path.join(\"./\", \"data\", \"house_tiny.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e0f1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入文件\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # 列名\n",
    "    f.write('NA,Pave,127500\\n')  # 每行表示一个数据样本\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e12c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf10dc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN 0    127500\n",
      "1    106000\n",
      "2    178100\n",
      "3    140000\n",
      "Name: Price, dtype: int64\n",
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0        True      False\n",
      "1       2.0       False       True\n",
      "2       4.0       False       True\n",
      "3       3.0       False       True\n"
     ]
    }
   ],
   "source": [
    "# Nan表示缺失值，为了处理缺失值，典型的方法包括插值法和删除法\n",
    "# 插值法用一个替代值来弥补缺失值，而删除法则直接忽略缺失值\n",
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]  # 获取索引\n",
    "inputs = inputs.fillna(inputs.mean(numeric_only = True))  # 插值法\n",
    "print(inputs, outputs)\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b458743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 1., 0.],\n",
      "        [2., 0., 1.],\n",
      "        [4., 0., 1.],\n",
      "        [3., 0., 1.]], dtype=torch.float64)\n",
      "tensor([127500., 106000., 178100., 140000.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 转为张量格式\n",
    "x = torch.tensor(inputs.to_numpy(dtype=float))\n",
    "y = torch.tensor(outputs.to_numpy(dtype=float))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3a4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]])\n",
      "tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n"
     ]
    }
   ],
   "source": [
    "# 线性代数\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
    "print(A)\n",
    "print(A + B)\n",
    "print(A * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5d65466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "tensor(190.)\n",
      "tensor([40., 45., 50., 55.])\n",
      "torch.Size([4])\n",
      "tensor([ 6., 22., 38., 54., 70.])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# 降维\n",
    "print(A.shape)\n",
    "print(A.sum())\n",
    "# 指定轴求和\n",
    "A_sum_axis0 = A.sum(axis=0)  # 轴0消失\n",
    "print(A_sum_axis0)\n",
    "print(A_sum_axis0.shape)\n",
    "A_sum_axis1 = A.sum(axis=1)  # 轴1消失\n",
    "print(A_sum_axis1)\n",
    "print(A_sum_axis1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b52ecfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5000)\n",
      "tensor(9.5000)\n",
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([ 8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "# 求均值\n",
    "print(A.mean())\n",
    "print(A.sum() / A.numel())\n",
    "print(A.mean(axis=0))\n",
    "print(A.sum(axis=0) / A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee59b6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
      "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
      "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
      "        [0.2286, 0.2429, 0.2571, 0.2714]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非降维求和\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "print(sum_A)\n",
    "print(A / sum_A)\n",
    "A.cumsum(axis=0)  # 累积求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77c9fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 点积，dot product\n",
    "x = torch.ones(4, dtype = torch.float32)\n",
    "y = torch.ones(4, dtype = torch.float32)\n",
    "print(torch.dot(x, y))\n",
    "print(torch.sum(x * y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3ab5149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 6., 22., 38., 54., 70.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量积\n",
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e333764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89e3c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "# L2范数\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "print(torch.norm(u))\n",
    "# L1范数\n",
    "print(torch.abs(u).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcd9f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度\n",
    "# 连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26f8cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 链式法则\n",
    "# 链式法则可以被用来微分复合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef38ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。\n",
    "\n",
    "# 导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。\n",
    "\n",
    "# 梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。\n",
    "\n",
    "# 链式法则可以用来微分复合函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15fb39a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor(42., grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 10., 20., 30.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量的反向传播\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)\n",
    "y = 2 * torch.dot(x, x)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "z = 3 * torch.dot(x, x)\n",
    "print(z)\n",
    "z.backward()  # 梯度发生了累加\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9e956d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()  # y = x1 + x2 + x3 + x4\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed1f41ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矢量的反向传播\n",
    "# 当调用向量的反向计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d94cf80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分离计算\n",
    "# 将某些计算移动到记录的计算图之外\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6ab3327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "897461cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python控制流的梯度计算\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e10ec059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PositiveDefiniteTransform', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
     ]
    }
   ],
   "source": [
    "# 查阅文档\n",
    "print(dir(torch.distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf6d4760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function ones in module torch:\n",
      "\n",
      "ones(...)\n",
      "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "    by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword arguments:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.ones(2, 3)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "    \n",
      "        >>> torch.ones(5)\n",
      "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92095f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edbdb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wz_py_3_8",
   "language": "python",
   "name": "wz_py_3_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
