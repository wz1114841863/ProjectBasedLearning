{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eea298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ce7169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0193,  0.1957,  0.0058,  0.1195,  0.0642, -0.2791,  0.1832,  0.0410,\n",
       "          0.1202,  0.0835],\n",
       "        [ 0.1011,  0.3723, -0.1108,  0.3213,  0.1109, -0.0227,  0.1090, -0.0565,\n",
       "          0.1628,  0.1060]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多层感知机\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    ")\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc56d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义块\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" 多层感知机实现 \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" 前向传播 \"\"\"\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988b1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2799, -0.1270, -0.0358,  0.1005, -0.0684,  0.0355, -0.1673,  0.3204,\n",
      "         -0.0537, -0.0223],\n",
      "        [-0.2764, -0.0953,  0.0438,  0.1827, -0.0887,  0.0115, -0.1569,  0.4264,\n",
      "         -0.1687,  0.0591]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2799, -0.1270, -0.0358,  0.1005, -0.0684,  0.0355, -0.1673,  0.3204,\n",
      "         -0.0537, -0.0223],\n",
      "        [-0.2764, -0.0953,  0.0438,  0.1827, -0.0887,  0.0115, -0.1569,  0.4264,\n",
      "         -0.1687,  0.0591]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "print(net(X))\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77dda8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    \"\"\" 顺序块 \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # _modules的主要优点是： 在模块的参数初始化过程中， \n",
    "            # 系统知道在_modules字典中查找需要初始化参数的子块。\n",
    "            self._modules[str(idx)] = module  ## _module的类型是OrderedDict\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\" 前向传播 \"\"\"\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29205f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1007,  0.0519, -0.1196, -0.1756,  0.0676,  0.0154,  0.2787, -0.0085,\n",
       "         -0.0165, -0.0911],\n",
       "        [-0.0992,  0.0866,  0.0042, -0.1856,  0.0131,  0.1237,  0.2857, -0.1415,\n",
       "         -0.0242,  0.0115]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    ")\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66596a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    \"\"\" 固定常量 \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 固定随机权重参数，其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" 前向传播 \"\"\"\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和num函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层,当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfc2ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1486, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41445800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0528, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9d03de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3891],\n",
       "        [0.4553]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数管理\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d64b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2880,  0.1276, -0.1241,  0.2443,  0.2370,  0.3315, -0.2618, -0.2396]])), ('bias', tensor([0.1385]))])\n"
     ]
    }
   ],
   "source": [
    "# 访问参数\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62eb59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1385], requires_grad=True)\n",
      "tensor([0.1385])\n"
     ]
    }
   ],
   "source": [
    "# 每个参数都表示为参数类的一个实例\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b3c4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数是复合的对象，包含值、梯度和额外信息\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce1aa3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n",
      "tensor([0.1385])\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问所有的参数\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n",
    "print(net.state_dict()['2.bias'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c9f7e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1317],\n",
       "        [-0.1318]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从嵌套块收集参数\n",
    "def block1():\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 4),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return net\n",
    "    \n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7150176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58fc1fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4213, -0.4938, -0.4728, -0.3758,  0.4617,  0.3971, -0.2798,  0.2159])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ba50205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0007,  0.0054,  0.0011, -0.0019]), tensor(0.))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数初始化\n",
    "# 默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    \n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e2f41a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7256a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0170,  0.2385, -0.5455,  0.2954])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9934125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight torch.Size([8, 4])\n",
      "init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -8.4307, -0.0000, -7.0876],\n",
       "        [-0.0000, -0.0000, -6.1127, -9.0216]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    \"\"\" 自定义初始化 \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"init\", *[(name, param.shape) \n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        \n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a2b6153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, -7.4307,  1.0000, -6.0876])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接赋值\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c806e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c0379",
   "metadata": {},
   "source": [
    "当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec22b4",
   "metadata": {},
   "source": [
    "延后初始化\n",
    "#深度学习框架无法判断网络的输入维度是什么。 这里的诀窍是框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "333ee22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27bd4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "821b0841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UninitializedParameter>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\wz_py_3_8\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(256),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(10),\n",
    ")\n",
    "print(net[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee01240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 20])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "y = net(X)\n",
    "\n",
    "print(net[0].weight.shape)\n",
    "print(net[2].weight.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adaae3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Module)  #@save\n",
    "def apply_init(self, inputs, init=None):\n",
    "    self.forward(*inputs)\n",
    "    if init is not None:\n",
    "        self.net.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a29318d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义层\n",
    "class CenteredLayer(nn.Module):\n",
    "    \"\"\" 构建一个没有参数的层 \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bfe3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82640fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2596e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc990404",
   "metadata": {},
   "source": [
    "下面我们继续定义具有参数的层， 这些参数可以通过训练进行调整。 我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。 比如管理访问、初始化、共享、保存和加载模型参数。 这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "114fbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \"\"\" 带参数的层 \"\"\"\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        print(self.weight.shape)\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "        print(self.bias.shape)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4193503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.3298,  1.1064,  2.2462],\n",
       "        [-0.1072,  1.2640, -0.5304],\n",
       "        [-1.8276,  0.1784,  0.3739],\n",
       "        [ 0.4672, -0.6441, -0.5024],\n",
       "        [ 0.8355,  0.9857, -0.6726]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "447b8ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.5648, 0.0000],\n",
       "        [0.3851, 2.8038, 0.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb01bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读写文件，加载和存储权重向量和整个模型\n",
    "x = torch.arange(4)\n",
    "torch.save(x, './data/x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f3d3565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.load('./data/x-file')\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "801bc475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'./data/x-file')\n",
    "x2, y2 = torch.load('./data/x-file')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8e3c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, './data/mydict')\n",
    "mydict2 = torch.load('./data/mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f258e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" 多层感知机 \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.output(F.relu(self.hidden(X)))\n",
    "    \n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e4acab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './data/mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0a71903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('./data/mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6149d2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7743a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 15:34:25 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.12                 Driver Version: 556.12         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   55C    P8             15W /  115W |     182MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3100    C+G   ...1.28.6010\\updated_web\\WXWorkWeb.exe      N/A      |\n",
      "|    0   N/A  N/A      4728    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "|    0   N/A  N/A      7796    C+G   ...0\\FlutterPlugins\\FlutterPlugins.exe      N/A      |\n",
      "|    0   N/A  N/A      9608    C+G   D:\\Snipaste\\Snipaste.exe                    N/A      |\n",
      "|    0   N/A  N/A     12480    C+G   ...C2024\\Acrobat\\acrocef_1\\AcroCEF.exe      N/A      |\n",
      "|    0   N/A  N/A     14220    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     15164    C+G   D:\\Obsidian\\Obsidian.exe                    N/A      |\n",
      "|    0   N/A  N/A     18956    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe      N/A      |\n",
      "|    0   N/A  N/A     24028    C+G   ...7\\extracted\\runtime\\WeChatAppEx.exe      N/A      |\n",
      "|    0   N/A  N/A     24284    C+G   ...b3d8bbwe\\Microsoft.Media.Player.exe      N/A      |\n",
      "|    0   N/A  N/A     28796    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     29228    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     29476    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12c856f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查询可用GPU数量\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f6c504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fadc9d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a97330e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0da18346",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44377f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2260],\n",
       "        [-0.2260]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36a4503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wz_py_3_8",
   "language": "python",
   "name": "wz_py_3_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
